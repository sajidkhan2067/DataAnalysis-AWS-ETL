# DataAnalysis-AWS-ETL

Key Observations:

- ðŸ§© A comprehensive set of services for constructing end-to-end data pipelines is provided by AWS, encompassing S3 for storage, AWS Glue for data cataloging and ETL, and Redshift for data warehousing.

- ðŸ§© The adoption of infrastructure as code enables the definition and deployment of AWS architecture through templates, ensuring consistency and scalability across projects.

- ðŸ§© An interactive development environment with Jupyter notebooks is facilitated by AWS Glue, allowing the writing and testing of PySpark ETL jobs before saving and scheduling for production.

- ðŸ§© Flexibility in data processing is provided by dynamic frames in AWS Glue, allowing transformations and aggregations using Spark-like syntax.

- ðŸ§© Redshift, acknowledged as a powerful data warehousing solution within AWS, is capable of loading and querying large volumes of data efficiently.

- ðŸ§© The automation of saving and scheduling ETL jobs within AWS Glue streamlines data pipelines, ensuring the reliable execution of data processing tasks on a regular basis.

- ðŸ§© The importance of diligent cleanup within the AWS stack after the completion of a data pipeline has been recognized to mitigate unnecessary costs and prevent prolonged resource consumption.



References: 
1. https://aws.amazon.com/glue/
2. https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/build-an-etl-service-pipeline-to-load-data-incrementally-from-amazon-s3-to-amazon-redshift-using-aws-glue.html
3. https://medium.com/analytics-vidhya/etl-data-pipeline-in-aws-150acd6fee60
4. https://github.com/AnandDedha/AWS/tree/main
